{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/satojkovic/ToT-Colab/blob/main/tree_of_thoughts_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Tree of Thoughts (ToT) Demo\n\nThis notebook demonstrates the implementation of the Tree of Thoughts algorithm.  \nPaper: [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)\n\n## Algorithm Overview\n\nTree of Thoughts differs from conventional single-shot reasoning by constructing a **tree structure of thoughts** to solve problems:\n\n1. **Thought Generation**: Generate multiple thought candidates at each step\n2. **State Evaluation**: Evaluate the value of each thought state\n3. **Selection**: Select the most promising thoughts for the next step\n4. **Search**: Systematically explore for optimal solutions using breadth-first search"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Environment Setup and Library Installation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries\n!pip install openai sympy pandas numpy matplotlib seaborn\n!pip install tree-of-thoughts-llm"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone official code (alternative)\n# !git clone https://github.com/princeton-nlp/tree-of-thought-llm.git\n# %cd tree-of-thought-llm\n# !pip install -e ."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport argparse\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Any\n\n# OpenAI API setup\nimport openai\nfrom google.colab import userdata\n\n# Set OpenAI API key (using Google Colab Secrets)\nos.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Basic Usage Example of Tree of Thoughts"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from tot.methods.bfs import solve\nfrom tot.tasks.game24 import Game24Task\n\n# Parameter configuration\nargs = argparse.Namespace(\n    backend='gpt-4',\n    temperature=0.7,\n    task='game24',\n    naive_run=False,\n    prompt_sample=None,\n    method_generate='propose',\n    method_evaluate='value',\n    method_select='greedy',\n    n_generate_sample=1,\n    n_evaluate_sample=3,\n    n_select_sample=5\n)\n\nprint(\"Tree of Thoughts configuration:\")\nprint(f\"- Model: {args.backend}\")\nprint(f\"- Thought generation: {args.method_generate}\")\nprint(f\"- State evaluation: {args.method_evaluate}\")\nprint(f\"- Selection method: {args.method_select}\")\nprint(f\"- Number of candidates: {args.n_select_sample}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Game24 Task Demonstration\n\nGame24 is a mathematical puzzle where you use four numbers to make 24."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Game24 task\ntask = Game24Task()\n\n# Display task details\nprint(f\"Dataset size: {len(task)}\")\nprint(f\"Example problem: {task.get_input(0)}\")\nprint(f\"Number of search steps: {task.steps}\")\n\n# Display first 5 problems\nprint(\"\\nExample problems:\")\nfor i in range(5):\n    print(f\"Problem {i+1}: {task.get_input(i)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solve single problem\nproblem_idx = 0  # Index of the problem to solve\ninput_numbers = task.get_input(problem_idx)\n\nprint(f\"Problem: {input_numbers}\")\nprint(\"Solving...\\n\")\n\n# Solve with ToT algorithm\nsolutions, info = solve(args, task, problem_idx)\n\nprint(\"\\n=== Solution Results ===\")\nfor i, solution in enumerate(solutions):\n    print(f\"Solution {i+1}:\")\n    print(solution)\n    print(f\"Correct: {task.test_output(problem_idx, solution)}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Algorithm Process Visualization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def visualize_search_process(info: Dict[str, Any]):\n    \"\"\"\n    Visualize the search process\n    \"\"\"\n    steps = info['steps']\n    \n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    fig.suptitle('Tree of Thoughts Search Process', fontsize=16)\n    \n    # 1. Number of candidates at each step\n    step_nums = []\n    candidate_counts = []\n    selected_counts = []\n    \n    for step_info in steps:\n        step_nums.append(step_info['step'])\n        candidate_counts.append(len(step_info['new_ys']))\n        selected_counts.append(len(step_info['select_new_ys']))\n    \n    axes[0, 0].bar(step_nums, candidate_counts, alpha=0.7, label='Generated candidates')\n    axes[0, 0].bar(step_nums, selected_counts, alpha=0.7, label='Selected candidates')\n    axes[0, 0].set_xlabel('Step')\n    axes[0, 0].set_ylabel('Number of candidates')\n    axes[0, 0].set_title('Number of candidates at each step')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # 2. Distribution of evaluation values\n    all_values = []\n    for step_info in steps:\n        all_values.extend(step_info['values'])\n    \n    axes[0, 1].hist(all_values, bins=20, alpha=0.7, edgecolor='black')\n    axes[0, 1].set_xlabel('Evaluation value')\n    axes[0, 1].set_ylabel('Frequency')\n    axes[0, 1].set_title('Distribution of evaluation values')\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    # 3. Highest evaluation value at each step\n    max_values = []\n    avg_values = []\n    \n    for step_info in steps:\n        values = step_info['values']\n        max_values.append(max(values) if values else 0)\n        avg_values.append(np.mean(values) if values else 0)\n    \n    axes[1, 0].plot(step_nums, max_values, marker='o', label='Maximum value')\n    axes[1, 0].plot(step_nums, avg_values, marker='s', label='Average value')\n    axes[1, 0].set_xlabel('Step')\n    axes[1, 0].set_ylabel('Evaluation value')\n    axes[1, 0].set_title('Evaluation value progression by step')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True, alpha=0.3)\n    \n    # 4. Evaluation values of selected candidates\n    selected_values = []\n    for step_info in steps:\n        values = step_info['values']\n        select_count = len(step_info['select_new_ys'])\n        if values:\n            sorted_values = sorted(values, reverse=True)\n            selected_values.append(sorted_values[:select_count])\n    \n    if selected_values:\n        for i, step_values in enumerate(selected_values):\n            axes[1, 1].scatter([i] * len(step_values), step_values, alpha=0.7)\n    \n    axes[1, 1].set_xlabel('Step')\n    axes[1, 1].set_ylabel('Evaluation value')\n    axes[1, 1].set_title('Evaluation values of selected candidates')\n    axes[1, 1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Visualize search process\nif 'info' in locals():\n    visualize_search_process(info)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Performance Evaluation on Multiple Problems"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_multiple_problems(task, args, start_idx=0, end_idx=10):\n    \"\"\"\n    Evaluate ToT performance on multiple problems\n    \"\"\"\n    results = []\n    \n    for i in range(start_idx, min(end_idx, len(task))):\n        print(f\"Problem {i+1}/{end_idx}: {task.get_input(i)}\")\n        \n        try:\n            solutions, info = solve(args, task, i, to_print=False)\n            \n            # Evaluate best solution\n            best_solution = solutions[0] if solutions else \"\"\n            test_result = task.test_output(i, best_solution)\n            \n            results.append({\n                'problem_idx': i,\n                'input': task.get_input(i),\n                'solution': best_solution,\n                'correct': test_result['r'],\n                'steps': len(info['steps']) if 'steps' in info else 0\n            })\n            \n            print(f\"Result: {'Correct' if test_result['r'] else 'Incorrect'}\")\n            print()\n            \n        except Exception as e:\n            print(f\"Error: {e}\")\n            results.append({\n                'problem_idx': i,\n                'input': task.get_input(i),\n                'solution': '',\n                'correct': 0,\n                'steps': 0\n            })\n    \n    return results\n\n# Run multiple problem evaluation\nprint(\"Starting performance evaluation on multiple problems...\")\nevaluation_results = evaluate_multiple_problems(task, args, start_idx=0, end_idx=5)\n\n# Analyze results\ncorrect_count = sum(1 for r in evaluation_results if r['correct'])\ntotal_count = len(evaluation_results)\naccuracy = correct_count / total_count if total_count > 0 else 0\n\nprint(f\"\\n=== Evaluation Results ===\")\nprint(f\"Correct answers: {correct_count}/{total_count}\")\nprint(f\"Accuracy: {accuracy:.2%}\")\n\n# Display results in DataFrame\ndf_results = pd.DataFrame(evaluation_results)\nprint(\"\\nDetailed results:\")\nprint(df_results[['problem_idx', 'input', 'correct']].to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Comparison of Different Algorithm Configurations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compare_configurations():\n    \"\"\"\n    Compare different ToT configurations\n    \"\"\"\n    configurations = [\n        {\n            'name': 'ToT (greedy)',\n            'method_generate': 'propose',\n            'method_evaluate': 'value',\n            'method_select': 'greedy',\n            'n_select_sample': 3\n        },\n        {\n            'name': 'ToT (sample)',\n            'method_generate': 'propose',\n            'method_evaluate': 'value',\n            'method_select': 'sample',\n            'n_select_sample': 3\n        },\n        {\n            'name': 'ToT (wide search)',\n            'method_generate': 'propose',\n            'method_evaluate': 'value',\n            'method_select': 'greedy',\n            'n_select_sample': 5\n        }\n    ]\n    \n    comparison_results = []\n    \n    for config in configurations:\n        print(f\"\\nConfiguration: {config['name']}\")\n        \n        # Update configuration\n        test_args = argparse.Namespace(**vars(args))\n        for key, value in config.items():\n            if key != 'name':\n                setattr(test_args, key, value)\n        \n        # Test with small sample\n        results = evaluate_multiple_problems(task, test_args, start_idx=0, end_idx=3)\n        \n        accuracy = sum(1 for r in results if r['correct']) / len(results)\n        comparison_results.append({\n            'configuration': config['name'],\n            'accuracy': accuracy,\n            'correct_count': sum(1 for r in results if r['correct']),\n            'total_count': len(results)\n        })\n        \n        print(f\"Accuracy: {accuracy:.2%}\")\n    \n    # Visualize results\n    df_comparison = pd.DataFrame(comparison_results)\n    \n    plt.figure(figsize=(10, 6))\n    plt.bar(df_comparison['configuration'], df_comparison['accuracy'])\n    plt.title('Performance Comparison of Different ToT Configurations')\n    plt.xlabel('Configuration')\n    plt.ylabel('Accuracy')\n    plt.xticks(rotation=45)\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n    return comparison_results\n\n# Run configuration comparison\nprint(\"Starting comparison of different algorithm configurations...\")\ncomparison_results = compare_configurations()\n\nprint(\"\\n=== Comparison Results ===\")\nfor result in comparison_results:\n    print(f\"{result['configuration']}: {result['accuracy']:.2%} ({result['correct_count']}/{result['total_count']})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Creating and Solving Custom Problems"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def solve_custom_problem(numbers_str: str):\n    \"\"\"\n    Solve custom problem\n    \"\"\"\n    print(f\"Custom problem: {numbers_str}\")\n    \n    # Create temporary task class\n    class CustomGame24Task(Game24Task):\n        def __init__(self, custom_input):\n            super().__init__()\n            self.custom_input = custom_input\n        \n        def get_input(self, idx):\n            return self.custom_input\n    \n    custom_task = CustomGame24Task(numbers_str)\n    \n    try:\n        solutions, info = solve(args, custom_task, 0)\n        \n        print(\"\\n=== Solution Results ===\")\n        for i, solution in enumerate(solutions):\n            print(f\"Solution {i+1}:\")\n            print(solution)\n            result = custom_task.test_output(0, solution)\n            print(f\"Correct: {result['r'] == 1}\")\n            print()\n        \n        return solutions, info\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n        return [], {}\n\n# Custom problem examples\ncustom_problems = [\n    \"1 2 3 4\",\n    \"4 1 8 7\",\n    \"2 3 5 6\"\n]\n\nprint(\"Solving custom problems:\")\nfor problem in custom_problems:\n    print(\"\\n\" + \"=\"*50)\n    solve_custom_problem(problem)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Summary and Future Directions\n\n### Features of Tree of Thoughts\n\n1. **Structured Search**: Constructs a tree structure of thoughts rather than single-shot reasoning\n2. **Intermediate State Evaluation**: Evaluates states at each thought step and selects optimal paths\n3. **Flexible Configuration**: Allows different strategies for generation, evaluation, and selection phases\n4. **High Problem-Solving Capability**: Provides systematic approaches to complex problems\n\n### Applicable Domains\n\n- **Mathematical Problem Solving**: Numerical calculation problems like Game24\n- **Creative Writing**: Novel and poetry composition\n- **Logic Puzzles**: Crossword puzzles and similar challenges\n- **Strategic Thinking**: Game theory and decision-making problems\n\n### Future Improvements\n\n1. **Efficiency**: Reducing computational costs and improving response times\n2. **Better Evaluation Functions**: More accurate intermediate state evaluation\n3. **Dynamic Search**: Adaptive search depth based on problem complexity\n4. **Multimodal Support**: Handling non-text inputs"
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMx8jQ9RwkzjVgLNcBEHOEH",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}